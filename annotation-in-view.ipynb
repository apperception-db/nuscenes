{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa92991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from pyquaternion import Quaternion\n",
    "import pandas as pd\n",
    "\n",
    "from utils.data_dirs import data_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a4d177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/apperception/data/raw/nuScenes/full-dataset-v1.0/Trainval'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = os.environ['NUSCENES_RAW_DATA']\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c846568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/apperception/data/raw/nuScenes/full-dataset-v1.0/Trainval\n",
      "/data/apperception-data/processed/nuscenes/full-dataset-v1.0/Trainval\n",
      "v1.0-trainval\n",
      "/work/apperception/data/raw/scenic/experiment_data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir, output_dir, folder, EXPERIMENT_DATA, suffix, *_ = data_dirs(False)\n",
    "print(base_dir)\n",
    "print(output_dir)\n",
    "print(folder)\n",
    "print(EXPERIMENT_DATA)\n",
    "print(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abc2a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/apperception/data/raw/nuScenes/full-dataset-v1.0/Trainval/v1.0-trainval'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dir = os.path.join(base_dir, folder)\n",
    "metadata_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7fb659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ego_pose.json\n",
      "scene_js.swp\n",
      "instance.json\n",
      "map.json\n",
      "sample_data.json\n",
      "visibility.json\n",
      ".scene.json.swx\n",
      "sample_annotation.json\n",
      "log.json\n",
      "sample.json\n",
      "sensor.json\n",
      "attribute.json\n",
      "scene_js.swo\n",
      "calibrated_sensor.json\n",
      ".scene.json.swp\n",
      "scene.json\n",
      "category.json\n"
     ]
    }
   ],
   "source": [
    "metadata_filenames = os.listdir(metadata_dir)\n",
    "print(\"\\n\".join(metadata_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff836fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(output_dir, 'annotation.pickle'), 'rb') as f:\n",
    "#     raw_annotations = pickle.load(f).to_dict('records')\n",
    "raw_annotations = pd.read_pickle(os.path.join(output_dir, 'annotation.pkl')).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe26e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(output_dir, 'sample_data.pickle'), 'rb') as f:\n",
    "#     raw_sample_data = pickle.load(f).to_dict('records')\n",
    "raw_sample_data = pd.read_pickle(os.path.join(output_dir, 'sample_data.pkl')).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42322ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_token': 'c36eb85918a84a788e236f5c9eef2b05',\n",
       " 'token': '2cd832644d09479389ed0785e5de85c9',\n",
       " 'instance_token': '5e2b6fd1fab74d04a79eefebbec357bb',\n",
       " 'translation': [993.884, 612.441, 0.675],\n",
       " 'size': [0.3, 0.291, 0.734],\n",
       " 'rotation': [-0.04208490861058176, 0.0, 0.0, 0.9991140377690821],\n",
       " 'category': 'movable_object.trafficcone',\n",
       " 'heading': 94.82400000058985,\n",
       " 'location': 'singapore-onenorth',\n",
       " 'scene_name': 'scene-0001',\n",
       " 'sample_data_tokens': ['7ad5d6b946ec4c8daf9ce2938e419ba7',\n",
       "  'e2310bef3dfe4872866df441a5ea4b1f',\n",
       "  'd04a6afe66064c689ec6a8a90771f793',\n",
       "  '99aed096956646a7a6c39aa35bc12c4d',\n",
       "  '3e1b12c693864739a2ead1130900c4b7',\n",
       "  '85ce4f3066e64d48bd3f1630391733c0']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e529c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from annotation token -> annotation\n",
    "annotation_map = {\n",
    "    a['token']: a\n",
    "    for a\n",
    "    in raw_annotations\n",
    "}\n",
    "assert len(annotation_map) == len(raw_annotations), (len(annotation_map), len(raw_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03842691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': '050322aacb9047f4bc2a734e800c0192',\n",
       " 'sample_token': '14d5adfe50bb4445bc3aa5fe607691a8',\n",
       " 'timestamp': 1531883530762460,\n",
       " 'is_key_frame': False,\n",
       " 'filename': 'sweeps/CAM_FRONT/n015-2018-07-18-11-07-57+0800__CAM_FRONT__1531883530762460.jpg',\n",
       " 'sample_timestamp': 1531883530949817,\n",
       " 'camera_translation': array([1010.28536588,  613.79584851,    1.55080497]),\n",
       " 'camera_rotation': array([-0.71340144,  0.7006582 , -0.00763616,  0.00884084]),\n",
       " 'camera_intrinsic': [[1266.417203046554, 0.0, 816.2670197447984],\n",
       "  [0.0, 1266.417203046554, 491.50706579294757],\n",
       "  [0.0, 0.0, 1.0]],\n",
       " 'ego_translation': [1010.2539305089451, 612.1315307276016, 0.0],\n",
       " 'ego_rotation': [-0.7172259417302351,\n",
       "  -0.008052965142693864,\n",
       "  0.008687339134831436,\n",
       "  -0.6967400005743498],\n",
       " 'scene_name': 'scene-0001',\n",
       " 'channel': 'CAM_FRONT',\n",
       " 'location': 'singapore-onenorth',\n",
       " 'ego_heading': -1.6592785295958965,\n",
       " 'camera_heading': -1.3335577747511875,\n",
       " 'frame_order': 26}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9883200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from sample data token (camera config token) -> sample data\n",
    "sample_data_map = {\n",
    "    sd['token']: sd\n",
    "    for sd\n",
    "    in raw_sample_data\n",
    "}\n",
    "assert len(sample_data_map) == len(raw_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f356f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in annotation_map.values():\n",
    "    for sdt in a['sample_data_tokens']:\n",
    "        assert sample_data_map[sdt]['sample_token'] == a['sample_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "688d38e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204894"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sd for sd in raw_sample_data if sd['is_key_frame']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3d5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_token_to_annotation_map = {}\n",
    "# for a in raw_annotations:\n",
    "#     sample_token = a['sample_token']\n",
    "#     if sample_token not in annotation_map:\n",
    "#         sample_token_to_annotation_map[sample_token] = []\n",
    "#     sample_token_to_annotation_map[sample_token].append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd3e7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenes = {}\n",
    "# for sd in raw_sample_data:\n",
    "#     scene_name = sd['scene_name']\n",
    "#     if scene_name not in scenes:\n",
    "#         scenes[scene_name] = {}\n",
    "#     scene = scenes[scene_name]\n",
    "\n",
    "#     sample_token = sd['sample_token']\n",
    "#     if sample_token not in scene:\n",
    "#         scene[sample_token] = []\n",
    "#     scene[sample_token].append(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0158b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*scenes.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6416bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*scenes['scene-0061'].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d054c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(scenes['scene-0061']['378a3a3e9af346308ab9dff8ced46d9c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa84f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find if each scene / camera has its key fame as its closest to the sample\n",
    "# closest_frame = {}\n",
    "# for scene_name, scene in scenes.items():\n",
    "#     for sample_token, sample in scene.items():\n",
    "#         camera = {}\n",
    "#         for s in sample:\n",
    "#             if s['channel'] not in camera:\n",
    "#                 camera[s['channel']] = []\n",
    "#             camera[s['channel']].append((\n",
    "#                 s['is_key_frame'],\n",
    "#                 s['timestamp'],\n",
    "#                 s['sample_timestamp'],\n",
    "#             ))\n",
    "        \n",
    "#         for c_name, _sample_data in camera.items():\n",
    "#             best_is_key_frame, best_timestamp, best_timestamp_diff = None, None, None\n",
    "#             for is_key_frame, timestamp, sample_timestamp in _sample_data:\n",
    "#                 timestamp_diff = sample_timestamp - timestamp\n",
    "#                 if best_timestamp_diff is None or abs(best_timestamp_diff) > abs(timestamp_diff):\n",
    "#                     best_is_key_frame, best_timestamp, best_timestamp_diff = is_key_frame, timestamp, timestamp_diff\n",
    "#             key = f'{sample_token}-{c_name}'\n",
    "#             assert key not in closest_frame\n",
    "#             closest_frame[key] = (best_is_key_frame, best_timestamp, best_timestamp_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68ce808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cf for cf in closest_frame.values() if not cf[0]]\n",
    "# # This filter is empty -> All the key frames are the one that is closest to the sample timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa0ce764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([(cf[2]) for cf in closest_frame.values()]) / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd5a95e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_token': 'c36eb85918a84a788e236f5c9eef2b05',\n",
       " 'token': '2cd832644d09479389ed0785e5de85c9',\n",
       " 'instance_token': '5e2b6fd1fab74d04a79eefebbec357bb',\n",
       " 'translation': [993.884, 612.441, 0.675],\n",
       " 'size': [0.3, 0.291, 0.734],\n",
       " 'rotation': [-0.04208490861058176, 0.0, 0.0, 0.9991140377690821],\n",
       " 'category': 'movable_object.trafficcone',\n",
       " 'heading': 94.82400000058985,\n",
       " 'location': 'singapore-onenorth',\n",
       " 'scene_name': 'scene-0001',\n",
       " 'sample_data_tokens': ['7ad5d6b946ec4c8daf9ce2938e419ba7',\n",
       "  'e2310bef3dfe4872866df441a5ea4b1f',\n",
       "  'd04a6afe66064c689ec6a8a90771f793',\n",
       "  '99aed096956646a7a6c39aa35bc12c4d',\n",
       "  '3e1b12c693864739a2ead1130900c4b7',\n",
       "  '85ce4f3066e64d48bd3f1630391733c0']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df3aed9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': '050322aacb9047f4bc2a734e800c0192',\n",
       " 'sample_token': '14d5adfe50bb4445bc3aa5fe607691a8',\n",
       " 'timestamp': 1531883530762460,\n",
       " 'is_key_frame': False,\n",
       " 'filename': 'sweeps/CAM_FRONT/n015-2018-07-18-11-07-57+0800__CAM_FRONT__1531883530762460.jpg',\n",
       " 'sample_timestamp': 1531883530949817,\n",
       " 'camera_translation': array([1010.28536588,  613.79584851,    1.55080497]),\n",
       " 'camera_rotation': array([-0.71340144,  0.7006582 , -0.00763616,  0.00884084]),\n",
       " 'camera_intrinsic': [[1266.417203046554, 0.0, 816.2670197447984],\n",
       "  [0.0, 1266.417203046554, 491.50706579294757],\n",
       "  [0.0, 0.0, 1.0]],\n",
       " 'ego_translation': [1010.2539305089451, 612.1315307276016, 0.0],\n",
       " 'ego_rotation': [-0.7172259417302351,\n",
       "  -0.008052965142693864,\n",
       "  0.008687339134831436,\n",
       "  -0.6967400005743498],\n",
       " 'scene_name': 'scene-0001',\n",
       " 'channel': 'CAM_FRONT',\n",
       " 'location': 'singapore-onenorth',\n",
       " 'ego_heading': -1.6592785295958965,\n",
       " 'camera_heading': -1.3335577747511875,\n",
       " 'frame_order': 26}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "378c2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world2pixel(annotation, sample_data):\n",
    "    ct = np.array(sample_data['camera_translation'])\n",
    "    cr = Quaternion(sample_data['camera_rotation'])\n",
    "    ci = np.array(sample_data['camera_intrinsic'])\n",
    "    at = np.array(annotation['translation'])\n",
    "\n",
    "    offset = (at - ct) # .reshape((3, 1))\n",
    "#     point_from_camera = np.dot(cr.unit.inverse.rotation_matrix, offset)\n",
    "    point_from_camera = cr.inverse.rotate(offset).reshape((3, 1))\n",
    "    if point_from_camera[2] < 0:\n",
    "        return np.array([-1, -1, 0])\n",
    "    assert point_from_camera.shape == (3, 1)\n",
    "    point2d = np.dot(ci, point_from_camera)\n",
    "    assert point2d.shape == (3,1)\n",
    "\n",
    "    return (point2d / point2d[2:3]).reshape((3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e20339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_view(annotation: \"dict[str, Any]\"):\n",
    "    def fn(sample_data_token: \"str\"):\n",
    "        sample_data = sample_data_map[sample_data_token]\n",
    "        point2d = world2pixel(annotation, sample_data)\n",
    "        x, y, _ = point2d\n",
    "        \n",
    "        _, _, _z = (Quaternion(sample_data['camera_rotation'])\n",
    "            .inverse\n",
    "            .rotate(np.array(annotation['translation']) - np.array(sample_data['camera_translation']))\n",
    "        )\n",
    "        return (0 <= x < 1600) and (0 <= y < 900) and _z > 0\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce05a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████████████████████████████████████████▋                                                                                                                                                                                                               | 239997/1166187 [09:51<43:44, 352.88it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m     channels \u001b[38;5;241m=\u001b[39m [sample_data_map[sdt][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sdt \u001b[38;5;129;01min\u001b[39;00m sample_data_tokens]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ma,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: sample_data_tokens,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_of_view_sample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;129;01mnot\u001b[39;00m in_view_a(x), a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m])],\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m: channels\n\u001b[1;32m     10\u001b[0m     }\n\u001b[0;32m---> 11\u001b[0m output_annotations \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     split(a)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m tqdm(raw_annotations)\n\u001b[1;32m     15\u001b[0m ]\n",
      "Cell \u001b[0;32mIn [26], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     channels \u001b[38;5;241m=\u001b[39m [sample_data_map[sdt][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sdt \u001b[38;5;129;01min\u001b[39;00m sample_data_tokens]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ma,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: sample_data_tokens,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_of_view_sample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;129;01mnot\u001b[39;00m in_view_a(x), a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m])],\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m: channels\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m output_annotations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 12\u001b[0m     \u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m tqdm(raw_annotations)\n\u001b[1;32m     15\u001b[0m ]\n",
      "Cell \u001b[0;32mIn [26], line 3\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(a):\n\u001b[1;32m      2\u001b[0m     in_view_a \u001b[38;5;241m=\u001b[39m in_view(a)\n\u001b[0;32m----> 3\u001b[0m     sample_data_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mfilter\u001b[39m(in_view_a, a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m      4\u001b[0m     channels \u001b[38;5;241m=\u001b[39m [sample_data_map[sdt][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sdt \u001b[38;5;129;01min\u001b[39;00m sample_data_tokens]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ma,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: sample_data_tokens,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_of_view_sample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;129;01mnot\u001b[39;00m in_view_a(x), a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m])],\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m: channels\n\u001b[1;32m     10\u001b[0m     }\n",
      "Cell \u001b[0;32mIn [25], line 4\u001b[0m, in \u001b[0;36min_view.<locals>.fn\u001b[0;34m(sample_data_token)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(sample_data_token: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m     sample_data \u001b[38;5;241m=\u001b[39m sample_data_map[sample_data_token]\n\u001b[0;32m----> 4\u001b[0m     point2d \u001b[38;5;241m=\u001b[39m \u001b[43mworld2pixel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     x, y, _ \u001b[38;5;241m=\u001b[39m point2d\n\u001b[1;32m      7\u001b[0m     _, _, _z \u001b[38;5;241m=\u001b[39m (Quaternion(sample_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera_rotation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39minverse\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mrotate(np\u001b[38;5;241m.\u001b[39marray(annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sample_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera_translation\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     10\u001b[0m     )\n",
      "Cell \u001b[0;32mIn [24], line 11\u001b[0m, in \u001b[0;36mworld2pixel\u001b[0;34m(annotation, sample_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m point_from_camera \u001b[38;5;241m=\u001b[39m cr\u001b[38;5;241m.\u001b[39minverse\u001b[38;5;241m.\u001b[39mrotate(offset)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m point_from_camera[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m point_from_camera\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m point2d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(ci, point_from_camera)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def split(a):\n",
    "    in_view_a = in_view(a)\n",
    "    sample_data_tokens = [*filter(in_view_a, a['sample_data_tokens'])]\n",
    "    channels = [sample_data_map[sdt]['channel'] for sdt in sample_data_tokens]\n",
    "    return {\n",
    "        **a,\n",
    "        'sample_data_tokens': sample_data_tokens,\n",
    "        'out_of_view_sample_data_tokens': [*filter(lambda x : not in_view_a(x), a['sample_data_tokens'])],\n",
    "        'channels': channels\n",
    "    }\n",
    "output_annotations = [\n",
    "    split(a)\n",
    "    for a\n",
    "    in tqdm(raw_annotations)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ed4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba67138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_to_annotations = {}\n",
    "sample_data_to_out_of_view_annotations = {}\n",
    "for o in output_annotations:\n",
    "    for sdt in o['sample_data_tokens']:\n",
    "        if sdt not in sample_data_to_annotations:\n",
    "            sample_data_to_annotations[sdt] = []\n",
    "        sample_data_to_annotations[sdt].append(o)\n",
    "    for sdt in o['out_of_view_sample_data_tokens']:\n",
    "        if sdt not in sample_data_to_out_of_view_annotations:\n",
    "            sample_data_to_out_of_view_annotations[sdt] = []\n",
    "        sample_data_to_out_of_view_annotations[sdt].append(o)\n",
    "print(len(sample_data_to_annotations))\n",
    "print(len(sample_data_to_out_of_view_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf885f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = {}\n",
    "for sd in raw_sample_data:\n",
    "    scene_name = sd['scene_name']\n",
    "    if scene_name not in scenes:\n",
    "        scenes[scene_name] = {}\n",
    "    scene = scenes[scene_name]\n",
    "    \n",
    "    channel = sd['channel']\n",
    "    if channel not in scene:\n",
    "        scene[channel] = []\n",
    "    scene[channel].append(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcecf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bird_eye_view(annotation, sample_data):\n",
    "    ct = np.array(sample_data['ego_translation'])\n",
    "    cr = Quaternion(sample_data['ego_rotation'])\n",
    "    at = np.array(annotation['translation'])\n",
    "    \n",
    "    offset = (at - ct)\n",
    "    point_from_ego = cr.inverse.rotate(offset)\n",
    "    return point_from_ego\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19764495",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_lines = [\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [0, 4],\n",
    "    [1, 5],\n",
    "    [1, 3],\n",
    "    [2, 3],\n",
    "    [2, 6],\n",
    "    [3, 7],\n",
    "    [4, 5],\n",
    "    [4, 6],\n",
    "    [5, 7],\n",
    "    [6, 7],\n",
    "    [4, 7],\n",
    "    [5, 6],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0f556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def annotate_videos():\n",
    "    shutil.rmtree('./output-videos')\n",
    "    os.mkdir('./output-videos')\n",
    "    \n",
    "    width = 1600\n",
    "    height = 900\n",
    "\n",
    "    mapx = 1400\n",
    "    mapy = 200\n",
    "\n",
    "    for scenename, scene in scenes.items():\n",
    "        for channel, sds in scene.items():\n",
    "            frames = sorted(sds, key=lambda x: x['timestamp'])\n",
    "            filename = f'annotated-{scenename}-{channel}.mp4'\n",
    "            print(filename)\n",
    "\n",
    "            out = cv2.VideoWriter(\n",
    "                os.path.join('output-videos', filename),\n",
    "                cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                10,\n",
    "                (width, height)\n",
    "            )\n",
    "\n",
    "            for frame in tqdm(frames):\n",
    "                if not frame['is_key_frame']:\n",
    "                    continue\n",
    "                imagefile = frame['filename']\n",
    "                img = cv2.imread(os.path.join(base_dir, imagefile))\n",
    "\n",
    "                sample_data_token = frame['token']\n",
    "                sd = sample_data_map[sample_data_token]\n",
    "\n",
    "                # Annotations BBox -> pixel\n",
    "                for a in sample_data_to_annotations.get(sample_data_token, []):\n",
    "                    if not in_view(a)(sample_data_token):\n",
    "                        raise Exception(a)\n",
    "\n",
    "                    size = (a['size'][1], a['size'][0], a['size'][2])\n",
    "                    _x0, _y0, _z0 = - (np.array(size) / 2)\n",
    "                    _x1, _y1, _z1 =   (np.array(size) / 2)\n",
    "                    _xs = [_x0, _x1]\n",
    "                    _ys = [_y0, _y1]\n",
    "                    _zs = [_z0, _z1]\n",
    "\n",
    "                    points = []\n",
    "                    for _x in _xs:\n",
    "                        for _y in _ys:\n",
    "                            for _z in _zs:\n",
    "                                _p = Quaternion(a['rotation']).rotate(np.array([_x, _y, _z]))\n",
    "                                p = world2pixel({\n",
    "                                    'translation': _p + a['translation'],\n",
    "                                    'category': a['category'],\n",
    "                                }, sd)[:2]\n",
    "\n",
    "                                points.append(p)\n",
    "\n",
    "                    if a['category'].startswith('vehicle'):\n",
    "                        color = (255, 0, 0)\n",
    "                    else:\n",
    "                        color = (0, 0, 255)\n",
    "                    for _p0, _p1 in bbox_lines:\n",
    "                        x0, y0 = points[_p0].astype(int)\n",
    "                        x1, y1 = points[_p1].astype(int)\n",
    "                        img = cv2.line(img, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "                # Plot All Annotations\n",
    "                for a in sample_data_to_annotations.get(sample_data_token, []) + sample_data_to_out_of_view_annotations.get(sample_data_token, []):\n",
    "                    white = False\n",
    "                    if not in_view(a)(sample_data_token):\n",
    "                        white = True\n",
    "\n",
    "                    point = bird_eye_view(a, sd)\n",
    "                    x, y, _ = (point * 3).astype(int)\n",
    "                    x += mapx\n",
    "                    y += mapy\n",
    "                    minx = max(0, min(1599, x - 1))\n",
    "                    miny = 899 - max(0, min(899, y + 1))\n",
    "                    maxx = max(0, min(1599, x + 1))\n",
    "                    maxy = 899 - max(0, min(899, y - 1))\n",
    "                    if white:\n",
    "                        img[miny:maxy, minx:maxx, 0] = 255\n",
    "                        img[miny:maxy, minx:maxx, 1] = 255\n",
    "                        img[miny:maxy, minx:maxx, 2] = 255\n",
    "                    elif a['category'].startswith('vehicle'):\n",
    "                        img[miny:maxy, minx:maxx, 0] = 255\n",
    "                        img[miny:maxy, minx:maxx, 1] = 0\n",
    "                        img[miny:maxy, minx:maxx, 2] = 0\n",
    "                    else:\n",
    "                        img[miny:maxy, minx:maxx, 0] = 0\n",
    "                        img[miny:maxy, minx:maxx, 1] = 0\n",
    "                        img[miny:maxy, minx:maxx, 2] = 255\n",
    "\n",
    "                # Plot Camera View Lines\n",
    "                for x, y in [(0, 0), (width, height), (width, 0), (0, height)]:\n",
    "                    [[fx, _, cx], [_, fy, cy], [_, _, s]] = sd['camera_intrinsic']\n",
    "                    _z = 1000\n",
    "                    _x = (s * x - cx) * _z / fx\n",
    "                    _y = (s * y - cy) * _z / fy\n",
    "\n",
    "                    xx, yy, _ = (Quaternion(sd['ego_rotation']).inverse.rotate(Quaternion(sd['camera_rotation']).rotate(np.array([_x, _y, _z]))) * 3).astype(int)\n",
    "\n",
    "                    xx += mapx\n",
    "                    yy += mapy\n",
    "\n",
    "                    origin = (mapx, 899 - mapy)\n",
    "\n",
    "                    img = cv2.line(img, origin, (xx, 899 - yy), (225, 225, 225), 2)\n",
    "\n",
    "                # Plot Ego Position\n",
    "                x = mapx\n",
    "                y = mapy\n",
    "                minx = max(0, min(1599, x - 1))\n",
    "                miny = 899 - max(0, min(899, y + 1))\n",
    "                maxx = max(0, min(1599, x + 1))\n",
    "                maxy = 899 - max(0, min(899, y - 1))\n",
    "\n",
    "                img[miny:maxy, minx:maxx, 0] = 0\n",
    "                img[miny:maxy, minx:maxx, 1] = 255\n",
    "                img[miny:maxy, minx:maxx, 2] = 0\n",
    "\n",
    "\n",
    "                # Plot Camera Position\n",
    "                ct = np.array(sd['ego_translation'])\n",
    "                cr = Quaternion(sd['ego_rotation'])\n",
    "                at = np.array(sd['camera_translation'])\n",
    "\n",
    "                offset = (at - ct)\n",
    "                x, y, _ = (cr.inverse.rotate(offset) * 3).astype(int)\n",
    "                x += mapx\n",
    "                y += mapy\n",
    "                minx = max(0, min(1599, x - 1))\n",
    "                miny = 899 - max(0, min(899, y + 1))\n",
    "                maxx = max(0, min(1599, x + 1))\n",
    "                maxy = 899 - max(0, min(899, y - 2))\n",
    "                img[miny:maxy, minx:maxx, 0] = 255\n",
    "                img[miny:maxy, minx:maxx, 1] = 0\n",
    "                img[miny:maxy, minx:maxx, 2] = 255\n",
    "\n",
    "                for _ in range(5):\n",
    "                    out.write(img)\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "# annotate_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f31ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(output_dir, 'partitioned_annotation.pickle'), 'wb') as f:\n",
    "#     pickle.dump(pd.DataFrame.from_dict(output_annotations), f)\n",
    "df_output_annotations = pd.DataFrame.from_dict(output_annotations)\n",
    "df_output_annotations.to_pickle(os.path.join(output_dir, 'annotation_partitioned.pkl'))\n",
    "#     raw_annotations = pickle.load(f).to_dict('records')\n",
    "\n",
    "df_output_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_split = {}\n",
    "\n",
    "for scene_name, scene in scenes.items():\n",
    "    for camera_name, camera_configs in scene.items():\n",
    "        print(scene_name, camera_name)\n",
    "        key = scene_name + '_' + camera_name\n",
    "        anns = []\n",
    "        for config in camera_configs:\n",
    "            token = config['token']\n",
    "            if not config['is_key_frame']:\n",
    "                continue\n",
    "            if token not in sample_data_to_annotations:\n",
    "                print(token, token in sample_data_to_out_of_view_annotations)\n",
    "                continue\n",
    "            anns.extend(sample_data_to_annotations[token])\n",
    "        def format_data(d):\n",
    "            _d = {**d, 'timestamp': config['timestamp']}\n",
    "            del _d['sample_token']\n",
    "            del _d['heading']\n",
    "            del _d['sample_data_tokens']\n",
    "            del _d['out_of_view_sample_data_tokens']\n",
    "            return _d\n",
    "        ground_truth_split[key] = [*map(format_data, anns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2957134",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(key, len(val)) for key, val in ground_truth_split.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(os.path.join(output_dir, 'annotation_splitted.json'), 'w') as f:\n",
    "    json.dump(ground_truth_split, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43905fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (apperception)",
   "language": "python",
   "name": "apperception"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
